# 机器学习中的决策树

> 原文:[https://www.tutorialandexample.com/decision-trees/](https://www.tutorialandexample.com/decision-trees/)

**决策树介绍**

决策树是基于监督学习的算法中最强大的分类算法之一。它被用作进行预测的工具，并且可以被结合到不同的领域中。在决策树的帮助下，可以根据不同的条件以不同的方式划分数据集。

决策树的主要实体是决策节点和树叶。决策节点是数据变得支离破碎的地方，而树叶是我们获得输出的地方。决策树的概念可用于回归和分类模型。

决策树是流程图的树状排列。下面给出了一个决策树示例:

<figure class="aligncenter">![Decision Trees Root Node](../Images/a1f6ee7188a1c7e06188c3329cccf5dc.png)</figure>

根据给定的图片，下面讨论了决策树使用的一些术语:

**根节点:**它是第一个节点，或者我们可以称它为父节点。它表示整个群体，并根据特征值分成两个或多个决策节点。

**决策节点:**将一个子节点拆分成更多数量的子节点，称为决策节点。

**叶节点:**这些是终端节点，因为它不能被进一步分割。

**子树:**一个分支是一棵完整树的细分。

### 决策树的优势:

1.  由于它不需要任何阅读和解释数据的先决统计知识，它甚至使来自非技术背景的用户更容易理解输出。
2.  它工作最快，找到最重要的变量以及变量之间的关系。基于决策树的预测能力，可以在决策树的帮助下创建新的变量。
3.  决策树正确地完成了特征选择和变量筛选。
4.  用户准备数据只需很少的努力。
5.  它不受异常值和缺失值的影响，因此不需要清理数据。
6.  它与数据类型无关，这意味着它既可以处理分类数据，也可以处理数值数据。
7.  这是一个非参数模型。
8.  性能不会受到参数之间非线性关系的影响。

### 决策树的缺点:

1.  决策树的一个主要缺点是处理过度拟合的问题。
2.  在长时间处理连续变量时，很难将变量分类到不同的类别中，因为这可能导致树的信息丢失。
3.  决策树本质上是可变的，也就是说，数据的微小变化可以完全改变生成的数据。为了解决这些问题，需要进行增压和装袋。
4.  对于受支配的类，决策树学习器构建有偏向的树。
5.  与其他算法相比，它的预测精度较低。
6.  计算变得非常复杂，有如此多的班级记录。

现在，我们将看到决策树分类器如何将两个不同的类别区分为两个不同的类别，然后我们将使用与之前模型中使用的相同数据集(即 **Social_Network_Ads** )将其结果与之前的分类模型进行比较。我们将再次从导入库和执行数据预处理开始，就像在早期模型中所做的那样。

```
# Import the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd 

# Import the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values 

# Split the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test) 
```

在我们完成预处理步骤后，我们将使决策树分类器适合训练集。首先，我们将从 scikit learn 导入**决策树**库，并创建其变量，将其命名为**分类器**，这是决策树的对象。

我们将传递两个参数。**判据**和**随机 _ 状态**，等于 **0** 。对于标准，默认参数是“gini”，但我们将采用基于**熵**的决策树，因为最常见的决策树是基于熵的，例如，NLP 的最大熵。

然后，像往常一样，我们将分类器拟合到 **X_train** 和 **Y_train** ，以帮助分类器学习 X_train 和 Y_train 之间的相关性。

```
# Fit Decision Tree Classifier to the Training set
from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(X_train, y_train) 
```

接下来，我们将预测观察值，为此，我们将创建一个名为 **y_pred** 的变量，它是包含 **test_set** 结果预测的预测向量。然后，我们将使混淆矩阵以与先前模型中相同的方式执行。

```
# Predicting the Test set results
y_pred = classifier.predict(X_test)
# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred) 
```

输出:

<figure class="aligncenter">![Decision Tree classifier](../Images/ea437cd43b6d7ac74534e2f6f22717d2.png)</figure>

从上面给出的输出图像可以看出，我们在 **test_set** 上有九个不正确的预测。

最后，我们将可视化训练集和测试集结果，就像我们在以前的模型中所做的那样，只需绘制一个图表，将预测将购买 SUV 的用户的区域与预测将不购买 SUV 的用户的区域分开。

### 可视化训练集结果:

为了可视化训练集结果，我们将绘制一个图表，其中决策树分类器将预测将购买 SUV 的用户的**是**，以及将购买 SUV 的用户的**否**，与我们在之前的模型中所做的方式相同。

```
# Visualizing the Training set results
 from matplotlib.colors import ListedColormap
 X_set, y_set = X_train, y_train
 X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                      np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01)) 
 plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
              alpha = 0.75, cmap = ListedColormap(('red', 'green')))
 plt.xlim(X1.min(), X1.max())
 plt.ylim(X2.min(), X2.max())
 for i, j in enumerate(np.unique(y_set)):
     plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                 c = ListedColormap(('red', 'green'))(i), label = j)
 plt.title('Decision Tree Classification (Training set)')
 plt.xlabel('Age')
 plt.ylabel('Estimated Salary')
 plt.legend()
 plt.show() 
```

**输出:**

<figure class="aligncenter">![ prediction boundary comprise of only horizontal and vertical lines.](../Images/59e72a0d4a4a21b1efb448c7936ef9cf.png)</figure>

从上面给出的输出图像可以看出，这里，预测边界只包括水平线和垂直线。它根据独立变量(年龄和估计工资)的某些条件执行拆分。

它试图抓住每一个属于真实集合的用户。它不想跳过任何用户，即使是绿色区域的红色用户，反之亦然。由于它试图抓住每一个用户，这是一个**过度适应**的例子。

**可视化测试集结果:**

我们将以与之前相同的方式可视化测试集结果。

```
# Visualizing the Test set results
 from matplotlib.colors import ListedColormap
 X_set, y_set = X_test, y_test
 X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                      np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
 plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
              alpha = 0.75, cmap = ListedColormap(('red', 'green'))) 
 plt.xlim(X1.min(), X1.max())
 plt.ylim(X2.min(), X2.max())
 for i, j in enumerate(np.unique(y_set)):
     plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                 c = ListedColormap(('red', 'green'))(i), label = j)
 plt.title('Decision Tree Classification (Test set)') 
 plt.xlabel('Age')
 plt.ylabel('Estimated Salary')
 plt.legend()
 plt.show() 
```

**输出:**

<figure class="aligncenter">![visualize the Test Set results](../Images/2bc7854b22eea95784b3760be93b33f8.png)</figure>

从上面的图像可以看出，对于一些新的观察，它在绿色区域捕捉红色用户，反之亦然。但是我们可以说它在计算所有 9 个不正确的预测方面做得很好。这只是一棵预测树，但你可以简单地想象一下，如果你评估大约 500 棵甚至更多的决策树，结果会是什么，这是随机森林的一种情况，我们将在下一章学习。