# Java 中的网络爬虫

> 原文:[https://www.tutorialandexample.com/web-crawler-in-java](https://www.tutorialandexample.com/web-crawler-in-java)

在这篇文章中，你会知道 java 中的网络爬虫是什么，它的功能是什么。你也将能够理解在哪里实现它。这

## 网络爬虫定义

web crawler 本质上是一个应用程序，主要用于 web 导航和页面发现，以便可以找到和索引新的或新创建的页面。爬虫广泛而深入地探索以提取超链接，从各种种子网站或众所周知的 URL 开始。

广度优先搜索算法的网络爬虫是其最重要的应用之一。有向图应该能够代表整个互联网。

网络爬虫需要健壮和友好。在这里，礼貌包括遵守 robots.txt 指令和避免重复访问网站。“强健”一词指的是避免有害行为和蜘蛛网的能力。

下面是创建网络爬虫的步骤

*   在第一阶段，我们从 frontier 中选择一个 URL。
*   获取 URL 的 HTML 代码。
*   通过解释 HTML 代码，您可以获得其他 URL 的 URL。
*   验证该 URL 是否已被爬网。我们还确定我们是否已经看过相同的内容。然后，如果两个条件都不匹配，则将它们添加到同一个索引中。
*   检查每个提取的 URL 是否同意被检查(robots.txt，抓取频率)。

> **注意:**由于代理问题，这段代码不能在在线 IDE 上运行。尝试在本地计算机上运行。

## 数据爬行和数据抓取之间的区别

数据爬行和数据抓取都是数据处理中至关重要的概念。数据爬行需要处理大数据集，并创建一个定制的爬虫，它甚至可以到达最隐蔽的网页。从任何来源提取数据被称为数据搜集。

<figure class="wp-block-table">

| **数据抓取** | **数据抓取** |
| 数据仅通过数据爬行从 web 中提取。 | 从任何来源(包括 web)提取数据被称为数据搜集。 |
| 复制是数据爬行的基本组成部分。 | 复制并不总是数据搜集的一个元素。 |
| 大部分是在大卷上进行的。 | 任何规模，无论大小，都可以使用。 |
| 只需要一个爬行代理。 | 搜索爬虫和代理都是必要的。 |

</figure>

让我们用一个示例程序来理解这个概念

文件名:Webcrawl.java

```
// Example program for java web crawler
import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.net.URL;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Queue;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

//This class holds the methods needed by the web crawler
class WebCrowler {

	// to keep the Links in the BFS  required FIFO sequence
	private Queue<String> queue;

	// To save visited Links
	private HashSet<String>
		discovered_websites;

	// Constructor for setting up the necessary variables
	public WebCrowler()
	{
		this.queue
			= new LinkedList<>();

		this.discovered_websites
			= new HashSet<>();
	}

	// a method to launch the BFS and find all URLs
	public void discover(String root)
	{
		// preserving the base URL to launch BFS
		this.queue.add(root);
		this.discovered_websites.add(root);

		// till the queue is empty, it will loop.
		while (!queue.isEmpty()) {

			// To save the URL that is now in the lead of the queue
			String v = queue.remove();

			// to save the website's unprocessed HTML
			String raw = readUrl(v);

			// Using regular expressions with URLs
			String regex
				= "https://(\\w+\\.)*(\\w+)";

			// To save the URL pattern created using regex
			Pattern pattern
				= Pattern.compile(regex);

			// To extract all the URL that
			// matches the pattern in raw
			Matcher matcher
				= pattern.matcher(raw);

			// It will loop until all the URLs
			// in the current website get stored
			// in the queue
			while (matcher.find()) {

				// To store the next URL in raw
				String actual = matcher.group();

				// It will check whether this URL is
				// visited or not
				if (!discovered_websites
						.contains(actual)) {

					// If not visited it will add
					// this URL in queue, print it
					// and mark it as visited
					discovered_websites
						.add(actual);
					System.out.println(
						"webpage accessed: "
						+ actual);

					queue.add(actual);
				}
			}
		}
	}

	// Function to return the raw HTML
	// of the current website
	public String readUrl(String v)
	{

		// Initializing empty string
		String raw = "";

		// If this code throws any exceptions, handle them in a try-catch statement.
		try {
			// Change the string in the URL
			URL url = new URL(v);

			// Website HTML should be read
			BufferedReader be
				= new BufferedReader(
					new InputStreamReader(
						url.openStream()));

			// In order to save website input
			String input = "";

			// line for line through the HTML and add it to raw
			while ((input
					= br.readLine())
				!= null) {
				raw += input;
			}
			br.close();
		}

		catch (Exception ex) {
			ex.printStackTrace();
		}

		return raw;
	}
}
public class Webcrawl {

	public static void main(String[] args)
	{
		//The object has been created
		WebCrowler web_crowler
			= new WebCrowler();
		String root
			= "https:// www.google.com";
		web_crowler.discover(root);
	}
}
```

**输出**

```
webpage accessed: https://www.google.com
webpage accessed: https://www.facebook.com
webpage accessed: https://www.amazon.com
webpage accessed: https://www.microsoft.com
webpage accessed: https://www.apple.com 
```