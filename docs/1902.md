# 负载平衡器-系统设计

> 原文：<https://www.tutorialandexample.com/load-balancer-system-design>

作为软件工程专业的学生或 IT 爱好者，我们大多数人都想知道这些庞大的 IT 应用程序是如何处理服务器上同时出现的数百万个请求的。每个成功系统背后的开发人员为管理其可伸缩性所做的艰苦工作基本上是未知的。当应用程序开始流行并且用户流量每天都在增加时，单台服务器上的负载也会增加，从而使应用程序变慢。使用通过在网络上添加各种机器来处理流量来扩展系统的漫长过程。

一旦网络上有多台服务器，就很难决定哪台服务器将处理哪种类型的请求。系统的全部数据和需求通过使用负载平衡器分布在整个系统中。系统中的负载平衡器决定哪个请求应该被路由到哪个服务器，而不会破坏所有现有机器之间的负载平衡。

负载平衡是系统设计领域中一个关键且广泛使用的主题；这是一个常见的话题，可能会在任何 it 公司的技术面试中被问到。现在让我们详细了解一下负载平衡的基础、类型和算法…

## 负载平衡

您可以将负载均衡器看作是站在服务器网络和请求流量之间的交通警察，决定哪个服务器应该路由哪个请求。它主要用于划分请求的总数，比如对数据库或高速缓存的读或写操作，这样就不会有一台服务器同时承担太多的请求。负载平衡器可以是硬件设备的物理部分，也可以是基于云的服务上的虚拟实例。

### 为什么我们需要负载平衡器？

要回答这个问题，试着想象一下当没有负载均衡器的系统遇到大量请求时会是什么情况。所有提交将直接连接到一个单一的服务器，没有负载平衡。这种系统模型将面临的主要问题是...

### 更高的失败几率

作为所有用户流量的唯一服务器，该机器更有可能崩溃或停机。如果发生这种情况，整个应用程序将会中断，用户将无法访问服务。简而言之，该系统更容易出现故障，对用户来说是不可用的，使其在用户中不太受欢迎，这对用户来说是一种损失。

### 超载的服务器

用户对特定服务器的请求是有限制的，它可以平稳地处理这些请求。一旦流量增加，请求不断增加，它可能会变慢，降低服务器的性能，甚至在负载变高时崩溃。这是服务器过载的问题，因此要解决这个问题，我们需要在服务器上添加更多的机器，并在网络上分发请求。

为了分发上面提到的请求，我们需要在互联网和我们机器的网络之间有一个负载平衡器。有了负载平衡器，我们可以通过在网络上添加 web 服务器并在它们之间平均分配流量来管理任意数量的用户请求。在这样的系统中，使用负载平衡器的好处是，即使任何服务器由于某种原因崩溃或离线，应用程序仍将继续提供服务。

此外，使用这种技术，执行每个请求所需的时间或延迟将会减少，因为每增加一台服务器，RAM、磁盘存储和处理单元也会增加。

负载平衡器最大限度地缩短响应时间，并最大限度地提高网络中每台服务器或机器的吞吐量。

系统设计中的负载平衡器确保系统始终可用，即使任何一个服务器离线，因为它只在网络上的在线服务器之间发送或分发请求。

负载平衡器通过执行连续的健康检查来跟踪每台服务器处理请求的能力。

负载平衡器还根据用户流量决定用于分发请求的服务器数量。

负载平衡器的另一个好处是，使用负载平衡器的系统比具有相同配置但没有负载平衡的系统更安全，因为在这种情况下，用户只知道负载平衡器的地址，而不知道实际的服务器。

## 在系统中定位负载平衡器

*   系统和服务器之间。
*   服务器和作业服务器之间。
*   在应用服务器和缓存服务器之间。
*   缓存和数据库服务器之间。

## 负载平衡器的类型

系统中的负载平衡是避免大多数不规则情况的必要条件；它可以通过以下三种方式实现，即

1.  硬件负载平衡器
2.  客户端的软件负载平衡器
3.  服务器端的软件负载平衡器

虽然名字暗示了每个负载平衡器是如何实现的，但是让我们来详细看看它们的基本实现的想法。

### 硬件负载平衡器

硬件负载平衡器是使用我们网络中的物理设备来实现的，以控制和管理网络上跨机器集群的流量。负载平衡器足够强大，能够处理所有类型的流量，如 HTTPS、HTTP、TCP 和 UDP。硬件负载平衡器通常被称为第 4-7 层负载路由器。

硬件负载平衡器通过提供指向负载平衡器的虚拟服务器地址而不是系统上的实际机器来保护网络。硬件负载平衡器是在服务器端实现的。在遇到任何请求时，它通过执行双向 NAT(网络地址转换)将请求转发给最合适的真实服务器。然而，硬件负载平衡器可以同时处理大量的流量，但它的灵活性有限，而且通常很昂贵。

硬件负载平衡器确保每台服务器都处于良好状态，并通过定期执行健康检查来做出响应。如果任何一台服务器显示出明显的迹象，它就会停止向该服务器发送流量，以维持系统的可用性。

获取和配置硬件负载平衡器的任务成本很高，这也是大多数

公司只在入口点使用硬件负载平衡器，并依靠其他类型在网络上转移或分发请求。

### 客户端的软件负载平衡器

负载平衡器不一定总是使用硬件设备来实现。有时，根据系统的需要，您可以将负载平衡器实现为应用程序客户端的软件逻辑。对于每个请求，负载均衡器会给客户机应用程序一个列表，列出所有连接到网络的可用或在线服务器；应用程序通常会从列表中选择第一个来执行所述请求。

如果系统面临所选服务器的问题，并且即使在设置的重试次数限制后仍持续出现故障，则逻辑会从列表中丢弃当前服务器，并将其标记为不可用。然后，应用程序选择列表中的下一个可用服务器。

软件负载平衡是在您的系统中实现负载平衡的最快和最便宜的方法之一。实现客户端软件负载平衡器比实现物理负载平衡器相对容易。

### 服务器端的软件负载平衡器

正如我们所看到的，负载平衡器基本上只是用来在服务器网络上分配用户流量的小代码片段。软件负载平衡器可以在机器网络或服务器启动之前在服务器端实现。服务器端的负载平衡器为您的设计提供了最大的灵活性。

因为它可以在任何标准设备(如 Windows 或 Linux)上实现，所以不需要安装任何特定的硬件设备，这使得它更加经济实惠，并且需要的维护更少。大多数公司更喜欢实现现成的负载均衡器，但是您总是可以选择自己实现它。

有许多应用程序可以帮助你直接在系统中安装负载平衡器。这些应用程序直接安装在服务器上，或者可在您的应用程序中用作服务(LBaaS)。云服务提供商 LBaaS 负责您系统中负载均衡器的顺利安装、升级和配置。

可以用来在系统中实现负载平衡的一些顶级软件如下:

1.  Nginx
2.  Avi Vantage 软件负载平衡器
3.  HAProxy
4.  肯普装卸工
5.  Loadbalancer.org
6.  管理人管理人
7.  Citrix ADC
8.  梭鱼负载平衡器 ADC
9.  胶囊内
10.  总正常运行时间云负载平衡器
11.  jetNEXUS 负载平衡器

## 负载平衡的类别

通常，负载平衡器分为三类

### 1.第 4 层(L4)负载平衡器

L4 负载平衡器也称为网络负载平衡器。网络负载平衡器使用来自第 4 层(网络)的信息来做出关于在特定机器上路由用户流量的决策。第 4 层负载平衡器帮助您有效地利用服务器，并最大限度地提高系统的可用性，因为它将用户流量分布在每个设备、交换机和路由器上。

L4 负载平衡器足够强大，可以在所有形式的 TCP 或 UDP 流量中每秒处理数百万个客户端请求。路由决策基于从 TCP 或 UDP 端口收到的网络数据包及其源目的地的 IP 地址。第 4 层负载平衡器也对网络数据包执行网络地址转换(NAT ),但它不会破坏或检查任何这些数据包的内容。

### 2.第 7 层(L7)负载平衡器

作为最古老的负载平衡形式，第 7 层负载平衡器通常被称为应用程序负载平衡器或 HTTP(S)负载平衡器。在 OSI 模型中，第 7 层处理应用层(HTTP / HTTPS)，负载平衡器在应用层决定路由不同的请求。

应用程序负载平衡器跟踪 HTTP 头、cookies、URL 和 SSL 会话细节以及 HTML 表单数据，以用于决定哪台服务器接受哪种类型的请求。

### 3.全球服务器负载平衡(GSLB)

许多应用程序托管在位于多个地理位置的基于云的数据中心；因此，许多组织试图转向负载平衡器，它能够以更高的可靠性和更少的延迟来交付这些应用程序，即使在任何远程设备或位置执行请求。对于有此类要求的 IT 公司来说，GSLB 是合适的选择。全局服务器负载平衡器扩展了几乎每个地理位置的 L4 和 L7 服务器的属性，以便它们能够在所有服务器或机器之间充分地分布数据。

当终端级客户端需要在任何虚拟工作区中的多个应用程序之间导航时，GSLB 还可以帮助他们，从而确保一致的体验。

## 负载平衡算法

有各种负载平衡算法来决定您的应用程序的路由过程，不同的公司根据他们的需求使用不同的算法。一些最常用的负载平衡算法如下:

### 1.一系列

这些请求以轮流的方式在网络中的所有机器或服务器之间按顺序划分。这种方法工作得很好，但是这种方法的问题是，在将任何请求路由到服务器之前，它不检查任何服务器的现有负载，因此服务器过载的风险仍然存在。

### 2.加权循环赛

这种方法几乎类似于循环算法。不过，在这种方法中，每个服务器都被分配了一个特定的权重，请求被路由，因此权重较高的服务器或机器比权重较低的服务器处理更多的请求。

### 3.最少联系方法

该算法解决了循环法中面临的问题。在这个算法中，我们首先检查与每个服务器相关联的活动连接或请求，然后新请求被路由到具有最少活动连接的机器。与 Roun-robin 算法相比，最少连接方法的成本稍高，但是由于决定是根据任何服务器上的现有负载做出的，因此与 Roun-robin 方法相比，应用程序失败的机会减少了。

### 4.最短响应时间法

这个算法比最小连接法稍微复杂一点。最短响应时间方法将请求路由到具有最少活动连接和最短平均响应时间的服务器。选择响应时间最短的服务器可以减少查找结果的延迟，从而提高应用程序的性能。

### 5.源 IP 哈希

在源 IP 哈希方法中，客户端的 IP 地址被视为决定请求将被路由到的服务器。客户端和接收计算实例的 IP 地址是用加密算法计算的，以便简化划分网络流量的过程。