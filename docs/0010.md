# 机器学习中的线性回归

> 原文:[https://www . tutorialandexample . com/linear-regression-tutorial/](https://www.tutorialandexample.com/linear-regression-tutorial/)

### 线性回归简介

线性回归是机器学习中最重要的统计算法，用于学习因变量和一个或多个独立特征之间的相关性。因此，我们可以说，两个变量之间的线性关系可以表述为因变量的值根据自变量的值的变化而变化(增加/减少)。

数学上可以写成:

Y=mX+C

其中 **Y** 为**因变量**称为**反应**， **m** 为**回归系数**， **X** 为**自变量**称为**预测因子**， **C** 为**常数**。 **C** 的值称为**截距**显示回归线与 Y 轴相交的点，m 计算评估回归线的斜率。

## 线性关系有两种类型:

1.**正线性关系:**如果因变量和自变量都增加，那么一种线性关系称为正线性关系。通过下面给出的图像可以更清楚地理解这一点:

<figure class="aligncenter">![Positive Linear Relationship](../Images/bd33484b857b23f09d3fa827001c0628.png)</figure>

2.**负线性关系:**随着自变量的增加，如果因变量减少，那么这种线性关系称为负线性关系。通过下面给出的图像可以更清楚地理解这一点:

<figure class="aligncenter">![Negative Linear Relationship](../Images/a6c1e733ff9d2a45aa9a6c08aca8b8f2.png)</figure>

### 线性回归的类型:

1)简单线性回归:

它是广泛使用的回归技术之一。解释这些结果很容易。它是线性回归的基本类型，根据单个特征预测结果。它假设它的两个变量是线性相关的。

为了部署简单线性回归模型，我们将经历以下一些步骤:

第一步:

第一步是数据预处理，就像我们之前做的那样。在此，我们将首先导入库，以获得帮助我们构建模型和数据集的基本工具。

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
dataset= pd.read_csv(' Salary_Data.csv') 
```

这是我们从上面的代码中得到的数据集:

<figure class="aligncenter">![Simple Linear Regression](../Images/f423695238d6f064503ce9b8198aec9f.png)</figure>

数据集包含两列；**经历**和**工资**的年份，是在公司工作的员工的信息。我们将训练我们的模型，该模型将学习每个雇员的经验年数和他们的工资之间的相关性。

之后，我们将把数据集分成训练集和测试集。

```
X= dataset.iloc [:,:-1].values
Y= dataset.iloc [: ,1].values
```

从上面给出的代码中，我们在变量浏览器中点击 **X** 或 **Y** 得到以下输出:

对于 X:

<figure class="aligncenter">![numpy array](../Images/5e64c14d74509b8e094e82c1e63ea911.png)</figure>

这里 X 是自变量的**矩阵特征**，Y 是因变量的**向量**。我们将不执行特征缩放，因为库会自己执行。

现在，我们将数据集分为训练集和测试集。随机状态参数被设置为 0，这样当算法中存在一些随机因素时，我们可以得到相同的结果。

```
from sklearn. model_selection import train_test_split
X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=1/3,random_state=0) 
```

在执行上述两行时，当我们在变量浏览器中单击 **X_train** 时，我们将得到以下输出:

<figure class="aligncenter">![x train Numpy array](../Images/a514dca65ffcab4f021878773b3bef8a.png)</figure>

这里 **X_train** 和 **Y_train** 组成**训练集**，而 **X_test** 和 **Y_test** 组成**测试集**。该模型将学习**训练集**的 X_train 和 Y_train 之间的相关性，以便在测试集上测试它们的预测能力。

第二步:

在这一步中，我们将首先使简单的线性回归算法适合训练集，为此，我们首先需要从 scikit learn 导入一个库 **linear_model** 。然后，我们将创建一个线性回归类的对象，这将是简单的线性回归。

```
from sklearn.linear_model import LinearRegression
regressor =LinearRegression()
regressor.fit(X_train, Y_train) 
```

**输出:**

```
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)
```

既然简单线性回归器已经知道了用户的经验数和他们的工资之间的关系，从而知道他们如何根据经验预测工资。

我们将创建一个预测值的**向量**，它将包含**测试集**薪水的**预测**，我们将在称为 **Y_pred** 的单个向量中看到所有这些薪水，这是因变量的向量预测。

```
 Y_pred=regressor.predict(X_test)
```

在执行这段代码时，一个名为 **Y_pred** 的变量将在 variable explorer 窗格中构建，包含测试用例中 10 个观察的预测工资。你可以点击 **Y_pred** 来查看。

**输出:**

通过比较 **Y_pred** 和 **Y_test** ，你可以计算实际工资和预测工资之间的差异，你也可以看到你的简单线性模型在做出预测方面有多好。

最后一步，我们将可视化训练集和测试集。为此，我们将绘制观察点和简单线性回归，以了解线性相关性以及简单线性模型对真实观察值的预测。为了绘制图表，我们将导入 matplot 库

### 训练集的可视化:

```
plt.scatter(X_train,Y_train, color='red')
plt.plot(X_train,regressor.predict(X_train),color='blue')
plt.title('Salary vs Experienced(Training set)')
plt.xlabel('Years of experience')
plt.ylabel('Salary')
plt.show() 
```

**输出:**

<figure class="aligncenter">![Visualization of the training set](../Images/15b8a8fe969d8dd35acec0deb1ac3665.png)</figure>

从上面呈现的图像中，雇员的真实值是红点，预测值(预测工资)在蓝色的简单回归线上。这表明工资和工作经验之间有明显的线性关系。

由于回归线非常接近所有的观测点，我们可以拟合一个简单的线性回归模型，给出一个很好的预测。从输出图像可以看出，实际工资与预测工资非常接近。

现在，我们将看到简单线性回归模型如何为新观察值(测试集)工作，因为该模型不是在测试集上训练的。我们现在将为测试用例绘图。

### 测试集的可视化:

```
plt.scatter(X_test,Y_test, color='red')
plt.plot(X_train,regressor.predict(X_train),color='blue')
plt.title('Salary vs Experienced(Training set)')
plt.xlabel('Years of expperience')
plt.ylabel('Salary')
plt.show() 
```

**输出**:

<figure class="aligncenter">![Visualization of Test set](../Images/e15c19320d8be3752fcfc8c60c533311.png)</figure>

这里，红点是测试集的观察点，蓝线是包含预测值的简单线性回归线。所做的预测相当好，因为我们已经做了很好的简单线性回归模型，它完全能够预测观察值。

### 2)多元线性回归

多元线性回归是简单线性回归的增强。使用两个或多个特征进行预测。

任何具有 n 个观察值、p 个独立变量和 y 作为响应相关变量的数据集，p 个特征的回归线在数学上可以写成:

f(x<sub>I</sub>)= m<sub>0</sub>+m<sub>1</sub>x<sub>i1</sub>+m<sub>2</sub>x<sub>I2</sub>+……+m<sub>p</sub>x<sub>IP</sub>

其中 F(X <sub>i</sub> 为预测值与 m <sub>0</sub> ，m <sub>1</sub> ，m <sub>2</sub> 的响应，..m <sub>p</sub> 是回归系数。

一般来说，它会在数据中添加一个称为残差的误差，并对等式进行如下修改:

f(x<sub>I</sub>)= m<sub>0</sub>+m<sub>1</sub>x<sub>i1</sub>+m<sub>2</sub>x<sub>I2</sub>+……+ m<sub>p</sub>x<sub>IP</sub>+e<sub>I</sub>

y<sub>I</sub>= F(x<sub>I</sub>)+e<sub>I</sub>

### 回归模型中的一些假设:

1.  **线性:**说明因变量和自变量之间的关系应该是线性的。
2.  **同方差性:**同方差性这个术语表示它应该保持恒定的方差。
3.  **多元正态:**多元回归假设残差呈正态分布。
4.  **缺乏多重共线性:**在这种情况下，假设数据中很少或没有多重共线性。

### 建立模型的几种方法:

*   多合一
*   反向淘汰
*   预选
*   双向消除
*   分数比较

### 反向消除:

步骤 1:开始的第一步是选择显著性水平(SL)。

第二步:现在用所有可能的预测来拟合整个模型。

第三步:寻找最高的 P 值。如果 P>SL，则转到步骤 4，否则模型准备就绪。

第四步:消除预测因素。

第五步:拟合模型，排除那个变量。

### 向前选择:

步骤 1:要输入模型，选择一个显著性水平(例如 SL = 0.05)。

第二步:现在拟合所有的简单回归模型，选择一个 P 值最低的模型。

第三步:保留这个变量，用一个已有的预测因子来拟合所有有希望的模型。

步骤 4:考虑具有最低 P 值的模型，如果 P < SL，则转到步骤 3，否则模型准备好了。

### MLR 模型的逐步实现:

第一步:数据预处理:

1.  导入库。
2.  导入数据集。
3.  编码分类数据。
4.  避免虚拟变量陷阱。
5.  将数据集拆分为训练集和测试集。

步骤 2:对训练集进行多元线性回归拟合。

第三步:预测测试集的结果。

### 数据预处理:

我们将首先导入库和数据集，就像我们在前面的步骤中所做的那样。

```
#importing the libraries 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
#importing the dataset using the Pandas library
dataset= pd.read_csv('50_Startups.csv') 
```

这里我们将使用美国 50 家初创公司的数据集。有特定财政年度的信息；一家公司在研发、管理、营销上花了多少钱，他们在某个特定的州赚了多少利润。

<figure class="wp-block-image">![information for a particular financial year](../Images/eb16a52b8d01490153450df10da7efdc.png)</figure>

现在我们将提取特征矩阵。这里， **X** 是**自变量**的矩阵， **Y** 是**因变量向量**的矩阵。

```
#extracting matrix features 
X= dataset.iloc [:,:-1]. values
Y= dataset.iloc [: , 4].values 
```

我们无法在**变量浏览器**中查看 **X** ，因为矩阵包含不同的类型。因此，它不能在这里选择单一类型。但是我们可以在 **IPython 控制台**中看到它的矩阵。

**输出:**

```
array([[165349.2, 136897.8, 471784.1, 'New York'],
        [162597.7, 151377.59, 443898.53, 'California'],
        [153441.51, 101145.55, 407934.54, 'Florida'],
        [144372.41, 118671.85, 383199.62, 'New York'],
        [142107.34, 91391.77, 366168.42, 'Florida'],
        [131876.9, 99814.71, 362861.36, 'New York'],
        [134615.46, 147198.87, 127716.82, 'California'],
        [130298.13, 145530.06, 323876.68, 'Florida'], 
        [120542.52, 148718.95, 311613.29, 'New York'],
        [123334.88, 108679.17, 304981.62, 'California'],
        [101913.08, 110594.11, 229160.95, 'Florida'],
        [100671.96, 91790.61, 249744.55, 'California'],
        [93863.75, 127320.38, 249839.44, 'Florida'],
        [91992.39, 135495.07, 252664.93, 'California'],
        [119943.24, 156547.42, 256512.92, 'Florida'],
        [114523.61, 122616.84, 261776.23, 'New York'], 
        [78013.11, 121597.55, 264346.06, 'California'],
        [94657.16, 145077.58, 282574.31, 'New York'],
        [91749.16, 114175.79, 294919.57, 'Florida'],
        [86419.7, 153514.11, 0.0, 'New York'],
        [76253.86, 113867.3, 298664.47, 'California'],
        [78389.47, 153773.43, 299737.29, 'New York'],
        [73994.56, 122782.75, 303319.26, 'Florida'],
        [67532.53, 105751.03, 304768.73, 'Florida'],
        [77044.01, 99281.34, 140574.81, 'New York'],
        [64664.71, 139553.16, 137962.62, 'California'],
        [75328.87, 144135.98, 134050.07, 'Florida'], 
        [72107.6, 127864.55, 353183.81, 'New York'],
        [66051.52, 182645.56, 118148.2, 'Florida'],
        [65605.48, 153032.06, 107138.38, 'New York'],
        [61994.48, 115641.28, 91131.24, 'Florida'],
        [61136.38, 152701.92, 88218.23, 'New York'],
        [63408.86, 129219.61, 46085.25, 'California'],
        [55493.95, 103057.49, 214634.81, 'Florida'],
        [46426.07, 157693.92, 210797.67, 'California'], 
        [46014.02, 85047.44, 205517.64, 'New York'],
        [28663.76, 127056.21, 201126.82, 'Florida'],
        [44069.95, 51283.14, 197029.42, 'California'],
        [20229.59, 65947.93, 185265.1, 'New York'],
        [38558.51, 82982.09, 174999.3, 'California'],
        [28754.33, 118546.05, 172795.67, 'California'], 
        [27892.92, 84710.77, 164470.71, 'Florida'],
        [23640.93, 96189.63, 148001.11, 'California'],
        [15505.73, 127382.3, 35534.17, 'New York'],
        [22177.74, 154806.14, 28334.72, 'California'],
        [1000.23, 124153.04, 1903.93, 'New York'],
        [1315.46, 115816.21, 297114.46, 'Florida'],
        [0.0, 135426.92, 0.0, 'California'], 
        [542.05, 51743.15, 0.0, 'New York'],
        [0.0, 116983.8, 45173.06, 'California']], dtype=object) 
```

从上面的输出中，您可以看到来自 R&D 支出、行政支出、营销支出以及州政府的四个自变量。要查看 **Y** ，我们可以在**变量浏览器**中轻松打开矩阵。

<figure class="aligncenter">![Variable Explorer](../Images/a205d53529bdbe747eeed78f9354dd9f.png)</figure>

在查看 **X** 时，您一定注意到了状态变量，这是一个分类数据，必须对它进行编码，因为它包含文本。因为如果只这样保持下去，会造成机器学习模型方程的问题。因此，为了对其进行编码，我们将采用与数据预处理步骤完全相同的方式。我们将使用 **LabelEncoder** 对 nos 中的特定列进行编码。为了消除关系顺序，我们将使用 **OneHotEncoder** 来创建**虚拟变量**。

```
# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder = LabelEncoder()
X[:, 3] = labelencoder.fit_transform(X[:, 3])
onehotencoder = OneHotEncoder(categorical_features = [3])
X = onehotencoder.fit_transform(X).toarray() 
```

因为我们所有的列都是同一类型的。独立矩阵的类型 **X** 现在是 **float64** 。我们可以在下图中看到它:

<figure class="aligncenter">![ independent matrix X is float64](../Images/5dbe7266beae328cc4083da6f1e8377a.png)</figure>

```
# Avoiding the Dummy Variable Trap
 X = X[:, 1:] 
```

我们正在删除第一列 **X** ，即从索引 0 开始的列。我们只包括从 1 到结尾的列，只是为了避免伪变量陷阱。

<figure class="aligncenter">![numpy](../Images/0f7208a16dcc8b686be724a8baa9f87a.png)</figure>

如果我们看上面给出的图像，我们会看到第一列已经被成功删除，这是为加利福尼亚州，下一次，我们将不需要手动执行。它将由 python 库来处理。

现在我们将数据集分成**训练集**和**测试集**。测试集将包含 10 个观察值，训练集将包含 40 个观察值。

```
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) 
```

现在，我们将像在线性回归模型中一样将多元线性回归拟合到定型集，因为我们仍在制作一个线性回归模型，但使用了几个独立变量。我们以与线性回归模型相同的方式进行。我们将使用 fit 方法来拟合训练集中的回归对象。

```
# Fitting Multiple Linear Regression to the Training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train) 
```

现在这个模型的下一步是测试集结果的预测。

```
# Predicting the Test set results
y_pred = regressor.predict(X_test) 
```

在执行上述代码时，我们将观察到测试集的以下预测值。我们也可以通过点击变量浏览器窗格中的 **Y_test** 来比较实际值和预测值。

<figure class="aligncenter">![Upon execution of the above code](../Images/21d7447914f1b5efa518512c5c4f6b92.png)</figure>

<figure class="aligncenter">![Upon execution of the above code](../Images/bea9781ce2b2e9c3355817c9bce8e2f2.png)</figure>

从上面的图像中，我们可以清楚地看到我们的模型做得很好，并且可以看到自变量和因变量之间存在多线性相关性。

现在我们将学习一些更好的方法来评估我们模型的性能。那么，你认为我们用给定的数据集做出了一个最佳模型吗？因为当我们建立这个模型时，我们实际上使用了所有的独立变量。但是，如果这些自变量中有一些具有高度的统计显著性，意味着它们对利润的因变量有很大的影响，而有一些根本没有统计显著性，这意味着即使我们从模型中移除这些变量，我们仍然会得到一些惊人的预测。

主要目标是找到一个最优的独立变量组，这样独立变量组中的每个变量对因变量利润都有很大的影响，也就是说，组中的每个独立变量都是一个强有力的预测器，具有很高的统计显著性，并对因变量利润有影响，因变量利润增加 1 个单位可能是正的，因变量利润减少 1 个单位可能是负的。为此，我们将引入逆向消去法。

## 反向消除步骤:

我们将首先导入 **statsmodel** 库。

```
import statsmodels.api as sm
```

我们现在将向矩阵 X 或独立变量矩阵添加一列 1，这将对应于与常数 b <sub>0</sub> 相关联的 **x <sub>0</sub> =1** 。为此，我们将使用 **append** 函数将列 1 添加到矩阵中。

```
X=np.append(arr = np.ones((50,1)).astype(int), values=X, axis=1)
```

因为我们想在矩阵的开头添加列 1，所以不是将列添加到矩阵，而是将矩阵添加到列。因为当我们把列添加到矩阵时，它是在最后添加的。所以，我们选择将矩阵添加到 1 的列中。这里我们使用 axis=0，因为我们想添加一条线。

我们现在要开始逆向淘汰了。

我们将首先创建一个新的特征矩阵 **X_opt** ，它将是该矩阵的最佳特征，即包含对因变量有很大影响的最佳独立变量组的矩阵。

```
X_opt=X[:, [0,1,2,3,4,5]]
regressor_OLS=sm.OLS(endog = Y, exog=X_opt).fit()
regressor_OLS.summary() 
```

然后，我们为 statsmodel 库的新类创建了一个新的回归对象。我们将普通的最小二乘算法拟合到 **X_opt** 和 **Y** 。为了得到自变量**的有效值，使用了汇总函数**。

**输出:**

<figure class="aligncenter">![variable summary function ](../Images/3f7adb20d5962fde038ecf8d1a39b634.png)</figure>

从上面给出的输出可以清楚地看到，我们得到了每个自变量的 **P 值**。我们现在将 P 值与显著性水平 **(SL=0.05)** 进行比较，以决定是否需要将其从我们的模型中移除。

这里 **x1** 和 **x2** 是**虚拟变量**， **x3** 是 **R & D** 花费， **x4** 是**行政花费**， **x5** 是**营销花费**。我们现在将从上面给出的输出中寻找 P 的最大值，我们可以得出它是 0.990 或 99%，远远大于 SL 值 5%。因此，我们需要移除预测器 x <sub>2</sub> ，它是状态的第二个<sup>和</sup>虚拟变量。

要删除它，我们将执行以下一些步骤:

```
X_opt=X[:, [0,1,3,4,5]]
 regressor_OLS=sm.OLS(endog = Y, exog=X_opt).fit()
 regressor_OLS.summary() 
```

**输出:**

<figure class="aligncenter">![Dummy Variable](../Images/de7b234c2f26b6c47f7af13121eefc92.png)</figure>

现在，最高值是 94% ,再次高于 SL 的 5%。我们将移除索引=1 时状态的**1<sup>1</sup>虚拟变量**的 **x1** ，并再次执行相同的步骤来寻找下一个最高的 P 值，直到我们得到预测利润的最佳团队。

```
 X_opt=X[:, [0,3,4,5]]
 regressor_OLS=sm.OLS(endog = Y, exog=X_opt).fit()
 regressor_OLS.summary() 
```

**输出:**

<figure class="aligncenter">![Dummy Variable](../Images/7c974c608a343541255c29068b47c78e.png)</figure>

现在，最高 P 值是 **60%** ，因此我们将删除 **x2** ，这是在 index=4 时的**管理支出**，并再次检查最高 P 值。

```
X_opt=X[:, [0,3,5]]
 regressor_OLS=sm.OLS(endog = Y, exog=X_opt).fit()
 regressor_OLS.summary() 
```

我们的模型终于准备好了，可能是指数 3 & 5，这是市场营销和 R&D 支出组成的自变量，使最好的团队预测利润。

<figure class="aligncenter">![Dummy Variable](../Images/475355f052edf7337cc6a8d2ce1f8d22.png)</figure>

根据上面给出的输出图像，我们需要查看表格并确定最高的 P 值。最高的 P 值在这里是 0.060 和 0.000，也就是 x1。P 值不能 b <sub>0</sub> 。它太小了，远低于 5%的 SL。

因此，R&D 支出是一个非常强有力的利润预测指标&肯定对因变量利润有很高的统计影响。

但是就第 **2 <sup>和第</sup>独立变量**而言，也就是指数为 5 的**营销支出**，P 值为 **0.060** ，略高于我们设定的 5%。所以，我们需要移除它。

```
X_opt=X[:, [0,3]]
 regressor_OLS=sm.OLS(endog = Y, exog=X_opt).fit()
 regressor_OLS.summary() 
```

**最终表格输出:**

<figure class="aligncenter">![powerful predictor](../Images/228147486688c9059f642c84daea6aab.png)</figure>

所以可以看出， **R & D 花**变量做了一个非常**有力的**利润**的预测器**。因此，最终，可以预测具有最高统计显著性和最强影响的利润的独立变量的最佳团队仅由一个独立变量组成，即 R & D spend。