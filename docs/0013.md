# 逻辑回归

> 原文:[https://www.tutorialandexample.com/logistic-regression/](https://www.tutorialandexample.com/logistic-regression/)

逻辑回归模型是一种监督学习模型，用于预测目标变量的可能性。因变量有两类，或者我们可以说它是二进制编码的 1 或 0，其中 1 代表是，0 代表否。

它是机器学习中最简单的算法之一。它将 P(Y=1)预测为 x 的函数。它可用于各种分类问题，如糖尿病检测、癌症检测和垃圾邮件检测。

### 逻辑回归的类型

二元目标变量的逻辑回归称为二元逻辑回归。目标变量可以分为两类或更多类，这是可以预测的。逻辑回归可以进一步分为以下几类:

1.二进制:在这种分类中，因变量将有两种情况之一；1 或 0，1 代表赢/是，0 代表输/否

*   多项式:是一种因变量可以有三个甚至更多无意义的无序类型的分类。它们表示 A 型、B 型和 C 型
*   序数:在此，因变量可以有三个或三个以上的有序类型或具有数量意义的类型。

### 逻辑回归中的假设

1.  在二元逻辑回归中，目标应该是二元的，结果用因子级别 1 表示。
2.  独立变量应该相互独立，在某种意义上说，模型中不应该有任何多重共线性。
3.  模型中只应包含有意义的变量。
4.  对于逻辑回归模型，要包括大样本量

二元逻辑回归模型

它是较简单的逻辑回归模型之一，其中因变量有两种形式；1 或 0。它对多个预测值/自变量和一个二元因变量之间的关系进行建模，以便发现最合适的模型。它通过最符合 logit 函数的数据来计算事件发生的概率。在这种情况下，线性函数用于作为另一个函数的输入，其数学表达式为:

y = b <sub>0</sub> +b <sub>1</sub> x

现在将 sigmoid 函数应用到这条线上；

<figure class="aligncenter">![Logistic Regression](../Images/971109689a1bb26c8f40c82620883eb5.png)</figure>

利用上述两个方程，我们可以推导出逻辑回归方程如下:

ln = p/(1-p)= b<sub>0</sub>+b<sub>1</sub>x

我们将会看到逻辑回归是如何设法分离某些类别并预测结果的。为此，我们将使用一个包含社交网络中用户信息的数据库，如用户 ID、年龄、性别和估计工资。社交网络有许多可以在社交网络上投放广告的客户。汽车公司的一名员工以低得离谱的价格推出了一款 SUV 汽车。

我们试图根据年龄和估计的工资变量来判断社交网络上的哪些用户会购买 SUV。因此，我们的特征矩阵将是年龄和估计工资。我们将找出他们之间的相关性，以及他们是否会购买。

我们将首先导入库和数据集，然后执行数据预处理步骤；

```
#import the library and the dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as mpt
dataset= pd.read_csv ('Social_Network_Ads.csv') 
```

导入数据后，您可以通过点击**变量浏览器**中的**数据集**来检查数据。这是下面给出的数据:

<figure class="aligncenter">![Logistic Regression 1](../Images/95c3ce23046c79260d774613ea19943f.png)</figure>

现在我们将提取特征矩阵和因变量矩阵。**特征矩阵**包含在**X**变量中，**因变量**矩阵保留在**Y**变量中。

```
#extracting matrix of independent variables and dependent variables
X= dataset.iloc [:,[2,3]]. values
Y= dataset.iloc [:, 4].values 
```

在执行上面两行时，下面给出了以下输出:

对于 X:

<figure class="aligncenter">![Logistic Regression 2](../Images/821f757ab6492d37271d66cf0000f3ce.png)</figure>

对于 Y:

<figure class="aligncenter">![Logistic Regression 3](../Images/45ae49e506008b332754c18bd9749533.png)</figure>

我们现在将数据集分为训练集和测试集。因为我们有 400 个观测值，所以一个好的测试规模应该是在**训练集**中的 **300 个**观测值和在**测试集**中剩余的 **100 个**观测值。然后，我们将应用功能缩放，因为我们希望准确的结果来预测哪些用户实际上会购买 SUV。

```
#splitting the dataset
from sklearn. model_selection import train_test_split
X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=0.25,random_state=0)
#feature scaling
from sklearn.preprocessing import StandardScaler 
sc_X=StandardScaler()
X_train=sc_X.fit_transform(X_train)
X_test=sc_X.transform(X_test) 
```

输出:

<figure class="aligncenter">![Logistic Regression 4](../Images/0ab3ff0e6ae46fbcc16bdcd77bca51d3.png)</figure>

<figure class="aligncenter">![Logistic Regression 5](../Images/cdea8cb726cfb0cfd45032efc9b93fdb.png)</figure>

从上面给出的图片中，可以清楚地看到 **X_train** 和 **X_test** 被很好地**缩放**，但是我们没有缩放 **Y_train** 和 **Y_test** ，因为它们由**分类数据**组成。

既然我们的数据经过了很好的预处理，我们就可以构建我们的逻辑回归模型了。我们将对训练集进行逻辑回归拟合。为此，我们将首先导入**线性模型**库，因为逻辑回归是线性分类器。由于我们在 2D 工作，我们的两类用户将被一条直线分开。

将创建一个新的变量**分类器**，它是一个逻辑回归对象，为了创建它，将调用一个**逻辑回归**类。我们将只包括**随机状态**参数以得到相同的结果。然后，我们将采用分类器对象，并使用 fit()方法将其拟合到训练集，以便分类器可以了解 X_train 和 Y_train 之间的相关性。

```
#fitting Logistic regression to the training set
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=0)
classifier.fit(X_train, Y_train) 
```

在学习了相关性之后，分类器现在将能够预测新的观察结果。为了测试它的预测能力，我们将使用测试集。一个新的变量 **y_pred** 将被引入，因为它将成为预测的向量。我们将使用逻辑回归类的 **predict()** 方法，在这种情况下，我们将通过**的 X_test** 论证。

```
#predicting the test set results
y_pred=classifier.predict(X_test) 
```

输出:

这是我们在实现上述代码行后得到的输出:

<figure class="aligncenter">![Logistic Regression 6](../Images/de35b7fc70d7860abfdccfd8a6aaca75.png)</figure>

现在，我们将评估我们的逻辑回归模型是否正确理解了训练集中的相关性，以了解它如何在新集或测试集上进行预测。我们将制作一个混淆矩阵，其中包含正确的预测以及我们的模型做出的错误预测。

因此，为此，我们将从 **sklearn.metrics** 库中导入一个函数。然后创建一个新的变量 **cm** ，我们将传递一些参数，例如； **Y_test** 是真实值的向量，表示用户是否真的购买了汽车， **Y_pred** 是预测向量，

```
from sklearn.metrics import confusion_matrix 
cm=confusion_matrix(Y_test, y_pred) 
```

输出:

```
array([[65,  3],
[ 8, 24]], dtype=int64) 
```

根据上面的输出，65+24=89 是正确的预测，而 3+8=11 是不正确的预测。

接下来，我们将对我们的结果进行图形可视化，其中我们将清楚地看到分类器的决策边界和决策区域。我们将制作一个图表，以便我们可以清楚地看到在用户将购买 SUV 的情况下，逻辑回归模型预测**是**的区域，以及在用户将不购买该产品的情况下**否**的区域。

为了可视化训练集的结果，我们将首先导入 **ListedColormap** 类来着色所有的数据点。然后我们会创建一些局部变量 **X_set** 和 **y_set** 来代替 **X_train** 和 **Y_train** 。命令 **np.meshgrid** 将帮助我们创建一个包含所有像素点的网格。我们已经将**最小**年龄值设为 **-1** ，因为我们不希望 out 点被挤压，并且**最大**值等于 **1** ，以获得我们希望包含在帧中的那些像素的范围，我们已经对薪水进行了同样的处理。我们采用的分辨率等于 0.01 T21。然后我们将使用 **contour()** 来制作两个预测区域之间的轮廓。之后我们将使用逻辑回归分类器的 **predict()** 来预测哪些像素点属于 0 和 1。然后如果像素点属于的**点，则着色为**红色**或者属于 **1** 的，则着色为**绿色**。**

```
# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, Y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step  =0.01),
np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
mpt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
alpha = 0.75, cmap = ListedColormap(('red', 'green')))
mpt.xlim(X1.min(), X1.max())
mpt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
mpt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
c = ListedColormap(('red', 'green'))(i), label = j)
mpt.title('Logistic Regression (Training set)')
mpt.xlabel('Age')
mpt.ylabel('Estimated Salary')
mpt.legend()
mpt.show() 
```

**输出:**

<figure class="aligncenter">![Logistic Regression 7](../Images/e0d5a8ee5c05e68602e758fbac6d87b7.png)</figure>

从上面给出的图表中，我们可以看到一些红色的点和绿色的点。所有这些点都是来自训练集的观察点，即这些都是被选择去训练集的**社交网络**的所有用户。并且这些用户中的每一个都由他们在 **X 轴**上的**年龄**和在 **Y 轴**上的**预计工资**来表征。

**红点**是因变量购买为**的训练集观测值，零**表示没有购买 SUV 的用户，**绿点**因变量购买等于**一**表示实际购买了 SUV 的用户。

根据上面给出的输出，在观察的基础上做出以下一些解释:

1.  我们可以看到，红色区域中估计工资较低的年轻人没有购买 SUV，因为这些是真正的观察点，而绿色区域中估计工资较高的老年人购买了 SUV。
2.  除此之外，可以看出，估计工资低的老年人，居然买了 SUV。
3.  而另一方面，我们可以看到那些买了 SUV 的估计工资很高的年轻人。

现在问题来了，分类的目的是什么？是什么造就了分类器？他们真正会做什么？

因此，这里的目标是将正确的用户分类到正确的类别中，这意味着我们正在尝试制作一个分类器，它将成功地将正确的用户分离到正确的类别中，并由**预测区域**表示。对于预测区域，我们指的是红色区域和绿色区域。对于红色区域中的每个用户，分类器预测愿意购买 SUV 的用户，对于绿色区域中的每个用户，分类器预测实际购买 SUV 的用户，使得这两个区域被称为预测边界的直线分开。

这里预测边界是一条直线，这意味着我们的逻辑回归分类器是一个线性分类器。由于我们的逻辑回归分类器是线性分类器，所以我们的预测边界将是直线，并且只是随机的。同样，如果我们在**三维**，那么预测边界将是一个**直平面**分隔两个空间。由于这是一个训练集，我们的分类器成功地学会了如何根据这些信息进行预测。

现在，我们将看到我们的逻辑回归分类器如何预测测试集，而我们的模型并不是基于该测试集构建的，并且是以与上一步相同的方式执行的。

```
# Visualising the Test set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_test, Y_test
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step  =0.01),
np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
mpt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
alpha = 0.75, cmap = ListedColormap(('red', 'green')))
mpt.xlim(X1.min(), X1.max())
mpt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
mpt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
c = ListedColormap(('red', 'green'))(i), label = j)
mpt.title('Logistic Regression (Test set)')
mpt.xlabel('Age')
mpt.ylabel('Estimated Salary')
mpt.legend()
mpt.show() 
```

**输出:**

<figure class="aligncenter">![Logistic Regression 8](../Images/9d69ad2a3a3856768c39e1353b609161.png)</figure>

从上面的输出图像可以看出，分类器所做的预测产生了很好的结果，并且预测得非常好，因为所有的红点都在红色区域中，但是只有少数绿点在红色区域中，这是可以接受的，不是大问题。这是由于我们在混淆矩阵中看到的 11 个不正确的预测，也可以从这里通过计算交替区域中出现的红色和绿色点来计数。可以看出，在红色区域，红色点表示没有购买 SUV 的人，在绿色区域表示购买了 SUV 的人。