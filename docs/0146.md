# 证明人工智能偏差的 5 种算法

> 原文：<https://www.tutorialandexample.com/5-algorithms-that-demonstrate-artificial-intelligence-bias>

不幸的是，在机器学习算法中，AI 偏差是由于算法开发过程中做出的偏见假设而产生的输出。由于以下两个原因，人工智能系统存在偏差，这些原因如下:

 <u>1.  认知偏差
2.  <u><u>完整数据不足</u></u>

 <u><u>当输入数据良好时，人工智能系统可以是健全的。这是一个绝望的事实，关于我们的社会，这是永久的偏见。它可能会反复发生，因为人类经常对宗教、性别、国籍和少数民族产生偏见，由于这种偏见，这种偏见是由于出生以来的社会家庭而形成的。因此，技术行业在向市场发布它们的人工智能算法之前，让它们没有偏见。公司可以这样做，以鼓励对无偏见人工智能的研究。**<u>T2】</u>**

 <u>有一些算法可以表明人工智能的偏见。这些偏见被用来反对少数民族，如黑人、妇女、穷人等。**<u>T2】</u>**

 <u>### 1.COMPAS 算法对黑人有偏见

COMPAS 代表**替代制裁的罪犯管理概况。** 它是由 Northpointe 开发并拥有的案件管理和决策支持工具，由美国法院使用。该 COMPAS 软件使用一种算法来检测潜在的累犯风险。在 COMPAS 的帮助下，法官决定一个罪犯是否应该受到惩罚。ProPublica 是一家新闻公司，它发现 COMPAS 有偏见。根据 ProPublica 的说法，COMPAS 让黑人比白人更有负罪感。白人被认为没有黑人危险，即使是暴力犯罪，这表明 COMPAS 是一种普遍存在于人类中的遗传偏见:黑人比白人能犯更多的罪。

 <u>### 2.PredPol 算法对少数民族有偏见

PredPol 代表**预测监管**。PredPol 是一种人工智能算法，旨在预测未来哪里会发生犯罪。通过收集犯罪数据，例如从特定位置的逮捕次数和报警次数，这是可能的。美国警察局使用这种算法。它的主要目的是通过承担人工智能的预测责任来减少警察部门的人为偏见。但是美国的一些研究人员发现 PredPol 是有偏见的。它一再派遣警察到特定的少数民族家庭，却看不到该地区的犯罪数量。这也证明了 PredPol 也有偏见。【T2

 <u>### 3.亚马逊的招聘引擎对女性有偏见

亚马逊的招聘引擎是为选择简历而创建的人工智能算法，然后他们会被调用进行面试和选择。开发这种算法是为了消除工作申请选择过程中的人为偏见。但是这个算法在选择简历时对这个女人有偏见。当亚马逊研究团队检查算法时，它自动选择了女性候选人。在那之后，亚马逊放弃了这种算法，不再用于评估候选人。**<u>T2】</u>**

 <u>### 4.谷歌照片算法对黑人有偏见

谷歌照片是一种人工智能算法，它为照片创建一个独立的部分，与图片中显示的内容相对应。该算法基于卷积神经网络(CNN)过程。在卷积神经网络的帮助下，它通过使用图像重组过程来标记照片。但是一个谷歌算法被发现，使得一群黑人和大猩猩在一起。然而，他们声称，他们对那个错误感到抱歉，今后再也不会重犯。但是图像调平过程仍然不精确。**<u>T2】</u>**

 <u>### 5.IDEMIA 的面部重组算法对黑人女性有偏见

IDEMIA 是一家为美国警察部门创建面部识别系统的公司，但当国家标准和技术研究所检查 IDEMIA 的面部识别算法时，他们发现与白人女性相比，它重复地将黑人女性识别为危险人物。但是 IDEMIA 并不总是能看到所有的脸。黑人女性的错误匹配案例高于白人女性。然后 IDEMIA 说国家标准技术研究所用的算法没有商业发布，他们的算法越来越好。**<u>T2】</u>**</u></u></u></u></u></u></u></u></u>