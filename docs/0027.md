# 机器学习中的决策树算法

> 原文：<https://www.tutorialandexample.com/decision-tree-algorithm-in-machine-learning>

如今我们非常熟悉机器学习这个词。机器学习技术无处不在。许多 IT 公司使用这种技术来改进他们的产品。决策树算法是机器学习模型中的重要算法之一。它可用于分类和回归技术。在这篇文章中，我们将探讨更多关于决策树算法。

## 什么是决策树算法？

树的基本结构是由节点和树枝组成的。在该决策树中将有根节点、叶节点和分支。它基本上是一个图形表示，根据不同的条件，对于一个给定的问题，所有可能的解决方案都可用。这主要是监督学习。它可用于分类和回归。但多用于分类问题。在树中，内部节点代表数据集的特征，树的分枝代表决策规则，叶子节点代表输出。我们可以使用分类和回归树算法来构建决策树。

**举例:**

让我们通过一个简单易行的例子来理解决策树的概念。假设一个候选人已经得到了一封录取通知书，而之前他也已经得到了另一份工作的录取通知书。现在他想决定他的公司的工作。这里我们可以使用决策树。所以，首先我们从根节点开始。在这个根节点中，我们检查工资是否为 3 万英镑及以上。如果分支说是，那么我们转到下一个节点，如果分支说否，那么我们可以说这个任务被拒绝。下一个节点可以检查位置。通过这种方式，您可以检查数据集的特征并对输入数据进行分类。

## 决策树算法中使用的术语

1.  **分支/子树:**一棵决策树中存在许多子树。这些子树构成了主决策树。
2.  **叶子节点:**从叶子这个词可以理解为这是我们决策树的结局。它主要表示决策树的最终输出。
3.  **父/子节点:**我们之前讨论过子树。在这个子树中，一个节点通过分支来自另一个节点。下一个节点称为子节点，前一个节点称为父节点。
4.  **修剪:**这是一个过程，我们可以通过它来最小化一棵树的分枝。
5.  **根节点:**是树开始的节点。它代表整个数据集。它被分成子树。
6.  **拆分:**是我们将节点划分为子节点的过程。

## 决策树算法的工作过程

现在，我们要去了解决策树算法的工作过程。在这种决策树算法中，采用输入数据集。之后，将输入数据的属性与现有数据集进行比较，然后选择分支。之后，我们去下一个节点。在下一个节点中，再次遵循相同的过程。这些事情不断重复，直到我们得到叶节点意味着我们的输出。你可以通过下面提到的这个算法来理解上面的过程:

*   **步骤 1:** 我们将通过初始化根节点来开始树。这个根节点将代表整个数据集。
*   **第二步:**之后，我们将从数据集中找到最佳属性。我们将使用属性选择措施。
*   **步骤 3:** 在下一步中，我们将把整个数据集分成可能拥有最佳属性的子集。
*   步骤 4: 之后，我们将在决策树中创建具有最佳属性的节点。
*   **步骤-5:** 我们将通过将数据集划分为子集并创建新的决策树节点来递归地完成这个过程。这个过程将继续下去，直到我们得到最好的结果。当我们不能在这个递归中更进一步时，我们将把那个节点声明为叶节点。这个叶节点将是我们的输出结果。

## 决策树算法的 Python 实现

现在，我们来看看决策树算法是如何用 Python 语言实现的。对于这个实现，我们将使用 user_data.csv 文件。按照下面给出的步骤实现决策树算法。

1.  数据预处理步骤。
2.  使决策树算法适合训练集。
3.  预测测试结果。
4.  测试结果的准确性(创建混淆矩阵)。
5.  可视化训练集结果。
6.  可视化测试集结果。

### 1.数据预处理步骤

下面是实现它的代码

```
1.	import numpy as nm  
2.	import matplotlib.pyplot as mtp  
3.	import pandas as pd  
4.	data_set= pd.read_csv('user_data.csv')  
5.	  
6.	x= data_set.iloc[:, [2,3]].values  
7.	y= data_set.iloc[:, 4].values  
8.	  
9.	from sklearn.model_selection import train_test_split  
10.	x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25, random_state=0)  
11.	  
12.	from sklearn.preprocessing import StandardScaler    
13.	st_x= StandardScaler()  
14.	x_train= st_x.fit_transform(x_train)    
15.	x_test= st_x.transform(x_test) 
```

### 2.将决策树算法拟合到训练集

现在，我们将在训练集中拟合决策树算法。为此，我们必须从 sklearn.tree 库中导入 **DecisionTreeClassifier** 类。下面是实现它的代码:

```
1.	From sklearn.tree import DecisionTreeClassifier  
2.	classifier= DecisionTreeClassifier(criterion='entropy', random_state=0)  
3.	classifier.fit(x_train, y_train) 
```

### 3.预测测试结果

现在，我们将预测测试结果。下面是实现它的代码:

```
1\. y_pred= classifier.predict(x_test)  
```

### 4.检查结果的测试准确性

现在，我们将检查测试结果是否正确。下面是实现它的代码:

```
1\. from sklearn.metrics import confusion_matrix  
2\. cm= confusion_matrix(y_test, y_pred) 
```

### 5.训练集结果的可视化

现在，我们将可视化训练集的结果。为此，我们将使用一个图表。下面是实现它的代码:

```
1.	from matplotlib.colors import ListedColormap  
2.	x_set, y_set = x_train, y_train  
3.	x1, x2 = nm.meshgrid(nm.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),  
4.	nm.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))  
5.	mtp.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),  
6.	alpha = 0.75, cmap = ListedColormap(('purple','green' )))  
7.	mtp.xlim(x1.min(), x1.max())  
8.	mtp.ylim(x2.min(), x2.max())  
9.	fori, j in enumerate(nm.unique(y_set)):  
10.	mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],  
11.	        c = ListedColormap(('purple', 'green'))(i), label = j)  
12.	mtp.title('Decision Tree Algorithm (Training set)')  
13.	mtp.xlabel('Age')  
14.	mtp.ylabel('Estimated Salary')  
15.	mtp.legend()  
16.	mtp.show() 
```

### 6.测试集结果的可视化

对于测试集结果的可视化，我们只需要实现我们在训练集结果的实现中使用的相同代码。这里，唯一的变化是训练集将由测试集实现。下面是实现它的代码:

```
1.	from matplotlib.colors import ListedColormap  
2.	x_set, y_set = x_test, y_test  
3.	x1, x2 = nm.meshgrid (nm.arange (start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  = 0.01 ),  
4.	nm.arange ( start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01 ) )  
5.	mtp.contourf ( x1, x2,  classifier.predict ( nm.array ( [ x1.ravel(), x2.ravel() ] ).T ).reshape (x1.shape) ,  
6.	alpha = 0.75, cmap = ListedColormap ( ( 'purple', 'green'  ) ) )  
7.	mtp.xlim ( x1.min(), x1.max() )  
8.	mtp.ylim( x2.min(), x2.max() )  
9.	fori, j in enumerate ( nm.unique( y_set ) ):  
10.	mtp.scatter (x_set [y_set == j, 0], x_set [y_set == j, 1],  
11.	        c = ListedColormap(('purple', 'green'))(i), label = j)  
12.	mtp.title('Decision Tree Algorithm(Test set)')  
13.	mtp.xlabel('Age')  
14.	mtp.ylabel('Estimated Salary')  
15.	mtp.legend()  
16.	mtp.show() 
```

### 决策树的优势

1.  这种决策树算法可以帮助你解决与决策相关的问题。
2.  在这个算法中，我们可以检查所有可能的结果。
3.  这种算法的工作过程与我们大脑在决策时的工作过程大致相同。因此，人类很容易理解其中的逻辑。
4.  这里，在这个算法中，我们需要更少的数据清理。

### 决策树的缺点

1.  如果我们有许多类别标签，那么决策树可能会变得非常复杂。
2.  决策树算法可能会面临过度拟合问题**。**为了解决这个问题，我们可以使用随机森林算法。
3.  正如我们在前面的例子和实现中看到的，决策树有许多层。所以，这可能有点复杂。