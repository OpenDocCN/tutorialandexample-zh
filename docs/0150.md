# 人工神经网络教程

> 原文:[https://www . tutorialandexample . com/artificial-neural-network-tutorial](https://www.tutorialandexample.com/artificial-neural-network-tutorial)

**人工神经网络**或**神经网络**是仿照人脑设计的。在特定的条件下，人类有思维来思考和执行任务，但是机器怎么能做到呢？为此，设计了人工大脑，称为神经网络。类似于人的大脑有传递信息的神经元；就像神经网络有节点来完成这项任务一样。节点是数学函数。

神经网络基于生物神经网络的结构和功能。神经网络本身基于输入和输出而改变或学习。流过网络的信息影响[人工神经网络](https://en.wikipedia.org/wiki/Artificial_neural_network)的结构，因为它的学习和改进性质。

**罗伯特·赫克特-尼尔森博士**将神经网络解释为:

***“由几个简单、高度互联的处理元件组成的计算系统，它们通过对外部输入的动态响应来处理信息。”*T3】**

### 人工神经网络的组件

**神经元**

**神经元**类似于生物神经元。神经元无非是激活功能。人工神经元或激活函数在执行分类任务时具有**【开启】**特性。

我们可以说当输入高于某个值时；输出应改变状态，即 0 到 1，-1 到 1，等等。sigmoid 函数是**人工神经网络**中常用的激活函数。

**F (Z) = 1/1+EXP (-Z)**

**节点**

生物神经元连接在层次网络中，一些神经元的输出是其他神经元的输入。这些网络被表示为节点的连接层。每个节点携带多个加权输入，并将这些输入的总和应用于神经元，并生成一个输出。

<figure class="wp-block-image">![Nodes](../Images/6182fabe31a5b227537efd05b438ef31.png)</figure>

**偏差**

在神经网络中，我们根据给定的输入(x)预测输出(y)。我们创建一个模型，即(MX + c)，它帮助我们预测产量。当我们训练模型时，它会自己找到常数 m 和 c 的适当值。

常数 c 是偏差。偏差有助于使模型最适合给定的数据。我们可以说偏见给了我们自由去表现最好。

**算法**

**神经网络中需要算法**。生物神经元具有自我理解和工作能力，但人工神经元如何以同样的方式工作？为此，有必要训练我们的人工神经网络。为此，使用了许多算法。每种算法都有不同的工作方式。

在我们的人工神经网络的训练中使用了五种算法

1.  **梯度下降**
2.  **牛顿法**
3.  **共轭梯度**
4.  **拟牛顿的**
5.  Levenberg Marquardt

### 人工神经网络的类型

神经网络的工作方式类似于人类神经系统的工作方式。有几种类型的神经网络。这些网络实现基于确定输出所需的一组参数和数学运算。

1.  **前馈神经网络(人工神经元)**

FNN 是最纯粹的人工神经网络形式，输入和数据只能单向传输。数据仅向前流动；这就是为什么它被称为**前馈神经网络**的原因。数据通过输入节点，从输出节点输出。节点不是循环连接的。它不需要有隐藏层。在 FNN，它不需要多层。它也可以是单层。

<figure class="wp-block-image">![Feedforward Neural Network](../Images/3412463ddf0337ba53f541995fec78d9.png)</figure>

它具有通过使用分类激活函数实现的前传播波。所有其他类型的神经网络都使用反向传播，但 FNN 不能。在 FNN，计算产品的输入和重量之和，然后将其输入输出。诸如人脸识别技术和计算机视觉技术都在 FNN 使用。

*   **径向基函数神经网络**

**RBFNN** 求一个点到中心的距离，认为它工作平稳。RBF 神经网络有两层。在内层，特征与径向基函数相结合。特征提供了一个在考虑中使用的输出。也可以使用欧几里得度量之外的其他度量。

**重拨基础功能**

*   我们定义一个受体 t。
*   面对地图绘制在受体周围。
*   对于径向基函数，通常使用高斯函数。所以我们可以定义径向距离 **r=||X-t||。**

**重拨功能** = **？(r) = exp (- r /2？)，**哪里？> 0

神经网络用于电力恢复系统。当今时代，电力系统的规模和复杂性都有所增加。这两个因素都增加了大停电的风险。停电后需要尽快可靠地恢复供电。

<figure class="wp-block-image">![Redial Basis Function ](../Images/dea0ab49b35895be48bd1342690021dc.png)</figure>

### 3.多层感知器

多层感知器有三层或更多层。不能线性分离的数据借助这个网络进行分类。该网络是完全连接的网络，这意味着每个节点都与下一层中的所有其他节点相连。在多层感知器中使用非线性激活函数。它的输入和输出层节点连接成一个有向图。这是一种深度学习方法，因此，它使用反向传播来训练网络。它广泛应用于语音识别和机器翻译技术中。

<figure class="aligncenter">![Multilayer Perceptron ](../Images/911664563dd536ee8b3d3182f07b2930.png)</figure>

### 4.卷积神经网络

在图像分类和图像识别中，卷积神经网络起着至关重要的作用，或者可以说它是 CNN 的主要类别。人脸识别；物体检测等。，是 CNN 广泛使用的一些领域。这与 FNN 相似，神经元中存在可学习的权重和偏差。

CNN 将图像作为输入，在某个类别下分类和处理，例如狗、猫、狮子、老虎等。众所周知，计算机将图像视为像素阵列，这取决于图片的分辨率。基于图像分辨率，它会看到 h * w * d，其中 h=高度 w=宽度，d=尺寸。例如，RGB 图像是矩阵的 6 * 6 * 3 阵列，灰度图像是矩阵的 4 * 4 * 3 阵列。

在 CNN 中，每个输入图像将通过一系列卷积层，以及池、全连接层、过滤器(也称为内核)。它可以应用软最大函数来分类概率值为 0 和 1 的对象。

<figure class="aligncenter">![Convolution Neural Network](../Images/73e547d03a31d92ef5ee8a729c90b235.png)</figure>

### 5.递归神经网络

递归神经网络是基于预测的。在这个神经网络中，特定层的输出被保存并反馈给输入。这将有助于预测该层的结果。在递归神经网络中，第一层以与 FNN 层相同的方式形成，在随后的层中，递归神经网络过程开始。

所有的输入和输出都是相互独立的。在某些情况下，RNN 需要预测句子的下一个单词，而这将取决于句子的前一个单词。RNN 流行的主要和最重要的特点，即隐藏状态。隐藏状态记住关于序列的信息。

<figure class="aligncenter">![Recurrent Neural Network](../Images/12cb1b3fe3fe07189e3f9c8a87fd65ac.png)</figure>

RNN 有一个存储器来存储计算后的结果。RNN 对每个输入使用相同的参数，对所有隐藏层或输入执行相同的任务，以产生输出。与其他神经网络不同，RNN 参数的复杂性较小。

### 6.模块化神经网络

在模块化神经网络中，几个不同的网络在功能上是独立的。在 MNN，任务被分成子任务，由几个网络执行。在计算过程中，网络之间不直接通信。

所有网络都独立工作以实现输出。组合网络比平面和无限制的网络更强大。中介取每个网络的输出；处理它们以产生最终输出。

<figure class="aligncenter">![Modular Neural Network ](../Images/cb55983cc4b89617923d163a1344e094.png)</figure>

**神经网络的训练**

**培训策略**是用来执行学习过程的过程。为了获得最小的可能损失，我们对神经网络应用一种训练策略。当我们搜索一组使神经网络适合数据集的参数时，就完成了。

在神经网络的训练中有两个不同的概念。

*   **损失指数**
*   **优化算法**

**损失指数**

**损失指数**是一个重要的概念，在神经网络的训练中起着至关重要的作用。损失指数定义了神经网络需要完成的任务。需要神经网络来学习表示质量的测量。损失指数提供了这一衡量标准。

我们必须选择两个不同的项，即**误差项**和**正则项**，来建立一个损失指数。

 ****损失指数=误差项+正则项**

*   **错误 _ 期限**

在损失表达式中，误差是衡量神经网络如何适应数据集中的训练实例的最重要的术语。神经网络中使用了几个误差。

*   **均方误差**

它的主要任务是计算数据集中目标和神经网络输出之间的平均平方误差。

<figure class="wp-block-image">![Mean squared error](../Images/7e35f26f7e4a4d634b3781c5822fec2b.png)</figure>

*   **归一化平方误差**

NSE 的主要任务是使用归一化系数划分数据集中的目标和神经网络输出之间的平方误差。只有当 NSE 的值为 1 时，神经网络才预测“平均”数据。如果该值为零，则表明数据的预测是完美的。

<figure class="wp-block-image">![normalized squared error](../Images/3a629e0e5ff8c86a07242e641c1b8b2a.png)</figure>

*   **加权平方误差**

在二元分类应用中，WSE 用于不平衡目标。它的主要任务是当正面和负面的数量相差很大时，给正面和负面的例子以不同的权重。

<figure class="wp-block-image">![normalization error ](../Images/5738bae7ef08e40feef337ff4b5c6980.png)</figure>

*   **闵可夫斯基错误**

在训练实例中，目标之间的差的和被提升到输出的指数(在 1 和 2 之间变化)。指数被称为**闵可夫斯基参数**，指数的默认值为 5。

<figure class="wp-block-image">![minkowski_error](../Images/f2701a2c2889b34dc99f6e5a4fb14e80.png)</figure>

*   **正则化项**

在对输入变量做了小的改变之后，它导致了输出的小变化；据说这个解是正则的。

*   **L1 正规化**

该方法包含神经网络中所有参数的绝对值之和。

<figure class="wp-block-image">![L1 Regularization](../Images/0096b5e3609bfe8c4d8bffce6b8ade3b.png)</figure>

*   **L2 正规化**

该方法包含神经网络中所有参数的平方和。

<figure class="wp-block-image">![L2 Regularization](../Images/b9fb371a549bee7b4f10e03a60d579ce.png)</figure>

**网络中的损失函数**取决于**自适应参数**。这些参数组合成单个 n 维权重向量 **w.**

让我们看一个描述损失函数 f (w)的图表。

<figure class="wp-block-image">![Loss function](../Images/375dd7b86763b7346fd8505114e405c3.png)</figure>

在上图中，在该点，w*最小损失函数出现。我们可以找到损失函数在任一点 a 的一阶和二阶导数。在梯度向量中分组的一阶导数的元素可以写成

<figure class="wp-block-image">![elements of the first derivatives ](../Images/2dcf0376bd01fee516a14ca7084f3a41.png)</figure>

对于 i= 0，1，n。

类似地，在 Hessian 矩阵中分组的二阶导数的元素可以写成

<figure class="wp-block-image">![Hessian matrix](../Images/cee1e785c729a2c873bdf79a37a5d3aa.png)</figure>

对于 I，j= 0，1…..

**优化算法**

 **优化算法是用于在神经网络中执行学习过程的过程。它也被称为优化器。有不同类型的优化算法。每种算法在速度、精度和内存需求方面都有不同的特征和性能。

### 梯度下降

**梯度下降算法**又称**最速下降算法**。这是最简单的算法，它需要来自梯度向量的信息。GD 算法是一阶方法。

<figure class="wp-block-image">![](../Images/b46d94dbcf28337b25bf7ffcad5488ad.png)</figure>

对于 i= 0，1...

培训过程的活动图如下所示。

<figure class="aligncenter">![Activity diagram](../Images/a7b67c1a24211c887b678b268d91f76d.png)</figure>

**GD 算法**当我们有一个大的神经网络以及数以千计的参数时，推荐使用。这背后的原因是 GD 存储的是梯度向量，而不是 Hessian 矩阵。

### 牛顿方法

**牛顿法**是二阶算法。它利用了海森矩阵。它的主要任务是利用损失函数的二阶导数寻找更好的训练方向。

牛顿法迭代如下。

***w***<sup>(I+1)</sup>=***w***<sup>(I)</sup>-***H***<sup>(I)-1</sup>。 ***g <sup>(一)</sup>***

对于 i = 0，1.....

这里， ***H*** <sup>(i)-1</sup> 。 ***g <sup>(i)</sup>*** 被称为牛顿一步。参数的变化可能趋向于最大值而不是最小值。下面是用牛顿法训练神经网络的示意图。参数的改进是通过获得训练方向和合适的训练速率来实现的。

<figure class="aligncenter">![improvement of the parameter ](../Images/7b2a8722535cc3b9cfe70d6bcfa7537a.png)</figure>

### 共轭梯度

**共轭梯度**工作在梯度下降和牛顿法之间。共轭梯度避免了与牛顿法所需的评估、Hessian 矩阵求逆和存储相关的信息需求。

在 CG 算法中，搜索是在共轭方向上进行的，这比梯度下降方向更快收敛。训练是在与 Hessian 矩阵相关的共轭方向上进行的。参数的改进是通过计算共轭训练方向，然后在该方向上计算合适的训练速率来实现的。

<figure class="aligncenter">![Conjugate gradient ](../Images/526c3ebf38ac58ba4e5633d16a5f398b.png)</figure>

### 拟牛顿法

就计算而言，应用牛顿法是非常昂贵的。要计算 Hessian 矩阵，需要做很多运算。为了解决这个缺点，发展了**拟牛顿法**。

它也被称为可变矩阵法。在算法的每次迭代中，它建立一个逆 hessian 的近似值，而不是直接计算 hessian。关于损失函数的一阶导数的信息用于计算近似值。

参数的改进是通过获得一个拟牛顿训练方向，然后找到一个满意的训练率。

<figure class="aligncenter">![Quasi-Newton Method](../Images/8fed7b36f6bb600e8fb0fe18c3e7f2d3.png)</figure>

### 莱文伯格·马夸特

**Levenberg Marquardt** 也称为阻尼最小二乘法。该算法被设计成专门与损失函数一起工作。该算法不计算海森矩阵。它与**雅可比矩阵**和**梯度向量**一起工作。

在 Levenberg Marquardt 中，第一步是找到损耗、梯度和 Hessian 近似，然后调整饺子参数。

<figure class="aligncenter">![Levenberg Marquardt](../Images/15ae5f1d8719a400492b03f45d16d8cc.png)</figure>

### 人工神经网络的优缺点

**人工神经网络的优势**

1.  它将信息存储在整个网络上，而不是数据库中。
2.  经过人工神经网络的训练，即使信息不完全，数据也能给出结果。
3.  即使人工神经网络的一个或多个单元被破坏，仍然会生成输出。
4.  ANN 具有分布式内存，有助于生成所需的输出。
5.  安能让机器变得可以学习。
6.  人工神经网络具有并行处理能力，这意味着它可以同时执行多项任务。

**安的缺点**

1.  根据它们的结构，它需要具有并行处理能力的处理器。
2.  网络无法解释的行为是 ANN 的主要问题。安没有给出一个线索，当它产生一个试探性的解决方案。
3.  没有提供确定 ANN 结构的具体规则。
4.  没有关于网络持续时间的信息。
5.  把问题展现给网络，太典型了。

**结论**

在本教程中，我们只解释了神经网络的基本概念。在神经网络中，除了反向传播之外，还有许多其他技术和算法。神经网络在图像处理和分类方面表现良好。

目前，在神经网络上，正在进行非常深入的研究。一旦你对基本概念和算法有了足够的了解，你可能想要探索强化学习、[深度学习](https://www.tutorialandexample.com/deep-learning-tutorial/)等。****