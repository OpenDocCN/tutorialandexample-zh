# k-均值聚类算法

> 原文:[https://www . tutorialandexample . com/k-means-clustering-algorithm/](https://www.tutorialandexample.com/k-means-clustering-algorithm/)

**K 均值聚类简介**

K-mean 聚类属于基于无监督学习的范畴，是一种基于数据中存在的一些相似性模式将未标记数据集划分为聚类的过程。给定一组 m 个具有某些特征和值的数据项，主要目标是将相似的数据模式分类成 k 个聚类。

数据点被分配给每个聚类，使得数据点和质心之间的平方距离的总和最小。聚类中的变化越小，聚类中包含的相似数据点就越多。

### K-均值聚类的工作原理

**第一步:**首先，识别一个集群的 k 号。

**步骤 2:** 接下来，对 k 个数据模式进行分类，并将它们中的每一个分配给特定的聚类。

**步骤 3:** 通过计算一个聚类中包含的所有数据点的平均值来计算每个聚类的质心。

**步骤 4:** 不断迭代步骤，直到找到最优质心。

### 用 Python 实现

像往常一样，我们的第一步是导入所有三个基本库，即使用数学的 **numpy** 、绘制漂亮图表的 **matplotlib** 和更容易导入和管理数据集的 **pandas** ，就像前面的模型一样。

```
# Import the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd 
```

导入库之后，下一步是在 **pandas** 库的帮助下导入数据集作为数据框。这里我们将导入一个商场的数据集。

```
# Import the dataset
dataset =pd.read_csv('Mall_Customers.csv')
```

**输出:**

<figure class="aligncenter">![Implementation in Python ](../Images/957974496355fa3162417ba338c9db62.png)</figure>

购物中心数据集包含已经订购了特定城市的购物中心会员卡的客户的信息。认购时，客户提供了自己的详细资料，如**性别**、**年龄**、**年收入**。由于用户使用同一张卡在商场购物，因此商场保留每个用户的购买历史，这就是我们如何获得最后一列，即**消费分数(1-100)** ，这是由商场根据一些标准为其每个客户计算的。

现在我们将根据**年收入**和**消费得分**将客户分成不同的组。因为我们不知道会有多少客户群，我们也没有这个问题的答案，所以这是一个特别的聚类问题。因此，我们将从 k-means 聚类算法开始，找出这些客户群可能是什么，为此，我们将构建一个由我们感兴趣的两列组成的数组，即位于**3<sup>rd</sup>T9】索引处的**年收入**和位于**4<sup>th</sup>T15】索引处的**支出分数**。****

```
X = dataset.iloc[:, [3, 4]].values
```

在变量资源管理器窗格中点击 **X** 可以可视化数组，我们将得到如下输出:

**输出:**

<figure class="aligncenter">![clicking on X at the variable explorer pane,](../Images/aeeab1e452e2780534dbc18adc611cc6.png)</figure>

既然我们已经完成了数据集的导入，我们将专门从 k-means 聚类开始。在 k-means 聚类中，我们需要选择聚类的数量，但是这里我们不知道要寻找多少个客户端的聚类。因此，我们将通过结合**肘方法**来为我们的问题找到一个最优的聚类数。

我们将首先从 **scikit learn** 导入 **KMeans** 类，然后绘制肘方法图。为此，我们将计算 10 个不同数量的簇的平方和。因为我们将有十个不同的迭代，我们将使用 for 循环为十个分类创建一个十个不同的分类平方和列表。

我们将从初始化名为 **wcss** 的列表开始，然后在范围 1 到 11 中为 **i** 开始循环，这样包括 1 个界限，排除 11 个界限，因为我们想要 10 个 wcss。在这个循环的每次迭代中，我们将做两件事:

*   首先，我们将使 k-means 算法适合我们的数据 X，为此我们将创建一个对象 **kmeans** 的类 **KMeans** ，在该类中，我们将传递几个参数；
*   **n 个簇**称为簇数
*   **init** ，这是随机初始化的方法。由于不想陷入随机初始化陷阱，我们选择了一个非常强大的方法 **k-means++** 来代替 random。
*   **max_iter，**它被称为在 k-means 算法运行时定义最终聚类的最大迭代次数。这里我们选择了默认值，即 300。
*   **n_init，**这是 k-means 算法将使用不同初始质心运行的次数。我们选择了默认值，即 10。
*   **random_state，**固定 k-means 过程的所有随机因子。

现在我们已经完成了 k-means 算法，我们现在将使用 **fit()** 方法来拟合数据 **X** 。

*   其次，我们将独立于 wcss 列表计算组内平方和。集群内平方和还有另一个名字叫做**惯性**。scikit 学习库为我们提供了一个**惯性**属性，它将在集群平方和内进行计算。

接下来，我们将绘制**肘** **方法**图，为此，我们将传递在**范围(1，11)** 内的 X 轴值，因为我们想要 10 个集群和 Y 轴值，即 **wcss** 。对于该图，我们将**标题**命名为"**肘方法**"，将 **xlabel** 命名为"**集群数量**"，将 **ylabel** 命名为" **wcss** "为了显示图形，使用了 **plt.show()** 方法。

```
# Use the elbow method to find the optimal number of clusters
 from sklearn.cluster import KMeans
 wcss = []
 for i in range(1, 11):
 kmeans = KMeans(n_clusters= i, init = 'k-means++', max_iter=300, n_init=10, random_state = 0)
kmeans.fit(X)
wcss.append(kmeans.inertia_)
 plt.plot(range(1, 11), wcss) 
 plt.title('The Elbow Method')
 plt.xlabel('Number of clusters')
 plt.ylabel('WCSS')
 plt.show() 
```

**输出:**

执行上述代码后，我们可以清楚地看到，我们得到了五个最佳集群，这可以从下面给出的肘图中看出。

<figure class="aligncenter">![applying the K-means algorithm](../Images/f1ce1fca93d6b415eded2d71d464ea0e.png)</figure>

既然我们有了正确的聚类数，我们就可以开始下一步了，即将 K-means 算法应用于数据集 X，并给出准确的聚类数。这一步类似于我们在循环中执行的前一步。唯一的区别是，现在我们将把 **n_cluster** 设为 5，保持其余的输入参数不变。

接下来，我们将把 **kmeans** 拟合到数据集 **X** ，为此，我们将使用**fit _ predict**方法，因为对于我们数据集的每个客户端，它将通过将其集群编号返回到我们命名为 **y_kmeans** 的单个向量中来告诉我们该客户端所属的集群。

```
# Fit K-Means to the dataset
 kmeans = KMeans(n_clusters = 5, init = 'k-means++', max_iter=300, n_init=10, random_state = 0)
 y_kmeans = kmeans.fit_predict(X) 
```

在执行上面两行之后，如果我们转到**变量浏览器**，我们将看到我们的集群编号的新向量被命名为 **y_kmean** 。如果我们将数据集的**与**的 y_kmeans** 进行比较，我们会看到 1 号客户属于聚类 4，2 号客户属于聚类 3，3 号客户属于聚类 4，依此类推。**

**输出:**

<figure class="aligncenter">![we go to Variable explorer,](../Images/c37d10aa471172ceeae5e8f4f2e6f7fb.png)</figure>

现在进入下一步，我们将绘制一个非常漂亮的图表，很好地展示我们的五个集群，从而将这些集群可视化。在这一步中，我们将绘制一个散点图，上面是我们要添加质心的所有观察结果，并且还将突出显示聚类。

我们将从编写 **plt.scatter** 开始绘制属于集群 1 的所有观察值。正如我们在 y_kmeans 向量中看到的，聚类号从 0 到 4，这意味着我们的第一个聚类对应于 y_kmeans=0。在 plt.scatter 中，我们将首先获取数据集 **X** ，通过写 **y_kmeans==0** 来指定我们想要属于**聚类 1** 的观察值，接下来我们将指定我们想要数据 X 的第一列。由于 python 中的索引是从 **0** 开始的，所以我们对**第一列**采取了相同的方法，并且我们已经给出了属于所有观察点的 **X 坐标**现在我们将对 Y 坐标做同样的操作。我们将写 y_kmeans==0 和 **1** ，因为它对应于数据 X 的**第二列**，这是 Y 坐标。此外，我们将数据点的大小设为 100，然后为集群选择一种颜色并为其指定一个标签。同一行连续重复四次，以便通过仅改变对应于各个聚类的索引、颜色和标签来对其他聚类进行同样的操作。

之后，我们将绘制相同观测点的质心。我们将使用聚类中心属性来返回质心的坐标。为了突出显示它们，我们将选择一个更大的尺寸，即 300，然后将标题' **Clusters of customers** '添加到图中，将 X 轴名称指定为'**年收入(k$)** '，Y 轴名称为**'支出分数(1-100)** '。由于我们已经指定了所有不同的标签，我们将添加图例。使用 plt.show()来显示绘图。

```
# Visualize the clusters
 plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Cluster 1')
 plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 100, c = 'blue', label = 'Cluster 2')
 plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Cluster 3')
 plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')
 plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')
 plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroids') 
 plt.title('Clusters of customers')
 plt.xlabel('Annual Income (k$)')
 plt.ylabel('Spending Score (1-100)')
 plt.legend()
 plt.show() 
```

**输出:**

<figure class="wp-block-image">![we have specified all the different labels](../Images/8f36094d9de87f4f966fcd6934a37887.png)</figure>

从上面的输出图像中，可以得出以下结论:

*   **聚类 1:** 聚类 1 的客户收入高，消费分低，也就是说这个聚类的客户收入非常高，但是花的钱不够。因此，客户可以被命名为**小心**。
*   **集群 2:** 该集群中的客户具有平均工资和平均消费分数。所以，这些客户群是**标准**。
*   **聚类 3:** 它由高收入和高消费分数的客户组成，这意味着该聚类应该是商场营销的主要潜力，以了解客户正在购买哪种产品。所以，我们可以把它命名为**目标**。
*   **聚类 4:** 下一个聚类是低收入但高消费得分的客户，因此他们可以被称为**粗心的**。
*   **第五类:**最后一类是低收入和低消费分数的客户，这意味着他们是**明智的**。

为了清楚起见，我们可以在相同的代码中根据观察相应地更改集群标签，这是更改标签后得到的以下输出；

<figure class="wp-block-image">![observations for our clarity](../Images/9cfc848cfd7807acbbb8b03f7b467a47.png)</figure>

可以清楚地看到，我们的代码是非常好的结构，简单，并按照每个人的要求做工作。因此，建议您可以将相同的代码用于任何其他具有不同数据集的业务问题。您只需要更改索引。此外，完整的代码并不意味着维度大于 2，因为它会阻碍最后一步(即可视化阶段)的执行。因此，在这种情况下，如果您需要将数据集的维度减少到二维，以执行最后一部分代码并绘制聚类。

### K 均值聚类的优势

1.  易于实现、健壮且易于理解。
2.  比层次聚类更快，具有大量变量。
3.  形成更紧密的集群。
4.  使用不同的数据集，结果会好得多。

### K-均值聚类的缺点

1.  很难找到 k 的值。
2.  数据的排序对输出有很大影响。
3.  对重缩放敏感。
4.  如果几何形状复杂，它不能很好地执行聚类。

### K-均值聚类的应用

1.  对我们正在处理的数据的组成有一个直觉。
2.  先聚类后预测:为不同的子组形成不同的聚类。
3.  它有效地整合了市场细分、天文学、计算机视觉、图像分割、图像压缩以及分析动态数据的最新趋势。